Machine learning methods have been developed and refined to the point that they now pose a serious challenge to classic statistical models in the area of forecasting. For example, \citeauthor*{ahmed} \parencite*{ahmed} came up with a large-scale comparison study based on the M3 competition dataset to compare the major machine learning models for time series forecasting. Exponential smoothing models are among the most frequently used forecasting methods. However, \textcite{bergmeir} argue that these models can be substantially outperformed for the purpose of forecasting by applying the bagging algorithm to a bootstrapped component of the initial time series to estimate an ensemble of exponential smoothing models and by combining the resulting point forecasts. 

\section{Preliminary Overview of Relevant Literature}

\citeauthor*{januschowski2020} \parencite*{januschowski2020} provide an overview of the spectrum of Machine learning and statistical methods and the boundaries and intersections in the context of forecasting. This distinction is a common way of classifying forecasting methods in the forecasting literature. However, the authors contend that there is no added value from such a classification of the methods and they encourage the scientific communities to collaborate more.
\textcite{maasoumi2010} present applications in Economics and Finance at the intersection of Statistical Learning and Econometrics. Inter alia, the authors analyze bagging and combination forecasts and find that bagging forecasts often deliver the lowest mean squared forecast errors.

\section{Planned Methodology and Research Design}

As suggested by \textcite{bergmeir}, a Box-Cox transformation can be applied to the series in order to stabilize the variance of the time series and subsequently an STL decomposition is employed to break the time series down into the trend, the seasonal part and the remainder. Bootstrapping the remainder of the series is done to achieve stationarity. Furthermore, \citeauthor*{galicia2019} \parencite*{galicia2019} suggest the use of an ensemble time series forecasting model consisting of three components, which are decision trees, gradient boosted trees and Random Forests. For this model, the ensemble weights are computed by weighted least squares. They found that the ensemble outperforms its individual components. The three ensemble components can thus be employed for the purpose of this paper.

